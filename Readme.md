# Нейросеть для автодополнения текстов.

В данном проекте рассмотренно создание сети на базе LSTM для автодоплнения текста. Проект содержит следующие этапы:

1. На первом этапенами был проанализирован и почищен датасет товитов было установленно, что как правило твиты не превышают 35 слов. так же нами была проведена токенизация текста и выделены тренировочные, валидационные и тестовые данные.
2. На втором этапе были созданны кастомные классы датасетов и функция collate_fn, такж был создан класс модели на базе архитектуры LSTM. Модель была обучена и проверено качество генерации текста. Модель можно попробовать доработать (возможно, уменьшение lr позволит сойтись до более низкого значения лосса, хотя и увеличит время обучения; так же стоит попробовать боле жёстко почистить исходные данные и убрать повторяющиеся знаки препинания, что позволит модели реже зацикливаться).
3. На третьем этапе была рассмотрена предобученная модель архитектуры трансформера, она генерирует более связный текст и на тестовых данных показала себя немного лучше чем LSTM, хотя она тоже иногда галлюционировала. 

## Структура проекта.
- **solution.ipynb** ноутбук с решением, который содержит исследование и код по предобработке данных, созданию и обучению сети.
- **data** папка с сырыми данными (отсутсвует в репозитории, поскольку после предобработки файлы занимают слишком много место и их нельзя залить на гит. Да и выкладывать данные в гит в целом плохая практика)
- **models** папка с сохранёнными весами модели.
- **src** папка со скриптами:
    - ***data_utils.py*** скрипт для предобработки данных. Функция *preprocessing_data* принимает на вход путь к сырым данным в формате .txt и пути для обработанных данных по которым сохраняет обработанные данные. Также функции можно передать имя модели для токенизации и рандом сид.
    - ***next_token_dataset.py*** скрипт для создания датасетов. Содержит класс Custom_Dataset и функцию collate_fn. При использовании необходимо использовать структуру (при необходимости внести изменения в параметры):

    ```python
    dataset = Custom_Dataset(df)
    dataloader = DataLoader(valid_dataset, batch_size=128, collate_fn=collate_fn, shuffle=False)
    ```

    - ***lstm_model.py*** - содержит класс LSTM модели для генерации текста. Функция generate данного класса принимает на вход токены и аттеншн маск (в формате батчей) и генерирует продолжение ответа, пока не достигнет max_len или токена конца последовательности.
    - ***eval_lstm.py*** - содержит функцию eval для оценки генерации с пмощи метрики rouge. На вход принимает модель для оценки, токенайзер (использованный для токенизации данных в даталоадере) и сам даталоадер (данные для генерации и оценки уже должны быть токенизированны).
    - ***lstm_train.py*** - содержит функции: *train_epoch* для тренировки одной эпохи, *eval* для оценки обучения на валидационной выборки, *train* для тренировки модели до max_epoch либо до того момента, когда лосс на валидации не будет улучшаться больше pathion эпох.
    - ***eval_transformer_pipeline.py*** - содержит скрипт для оценки генерации моделей на базе трансформера, на вход принимает имя модели и датафрем pandas со столбцами для генерации и оценки. Обратите вниемание, тексты уже должны быть токенезированны в данном датасете (после предобработки при помощи функции выше так оно и получится). Так же генерация текста при помощи трансформера может занять продолжительное время (примерно 11 секунд на 100 строк)
